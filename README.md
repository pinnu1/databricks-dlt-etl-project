# End-to-End Databricks ETL Pipeline using Delta Live Tables

## ğŸ“Œ Overview
This project implements a production-grade end-to-end ETL pipeline using
Azure SQL as the source and Databricks Delta Live Tables (DLT) following
the Medallion Architecture (Bronze, Silver, Gold).

The pipeline supports streaming + batch processing, data quality enforcement,
slowly changing dimensions, and BI-ready analytics.

---

## ğŸ§± Architecture
**Source â†’ Bronze â†’ Silver â†’ Gold â†’ BI**

- Source: Azure SQL (AdventureWorksLT)
- Processing: Databricks + Delta Live Tables
- Governance: Unity Catalog
- Analytics: Databricks SQL & Power BI
- Version Control: GitHub via Databricks Repos

---

## ğŸ¥‰ Bronze Layer
- Ingest data from Azure SQL using JDBC
- Incremental loading using ModifiedDate
- Stored as Delta tables in Unity Catalog

---

## ğŸ¥ˆ Silver Layer
- Generic data cleaning
- Column normalization
- Stream + batch hybrid joins
- Customer SCD Type-2 using DLT
- Data quality enforced using DLT expectations

---

## ğŸ¥‡ Gold Layer
- Business KPIs
- Daily revenue
- Revenue by product category
- Customer lifetime sales metrics
- Revenue by region

---

## ğŸ“Š BI & Reporting
- Databricks SQL Dashboards
- Power BI (DirectQuery)
- Materialized views for low-latency analytics

---

## ğŸš€ Key Features
- Delta Live Tables (DLT)
- Deterministic surrogate keys
- Data quality validation
- Medallion architecture
- BI-ready materialized views

---

## â–¶ï¸ How to Run
1. Configure Azure SQL credentials using Databricks Secrets
2. Create DLT pipelines using the provided Python files
3. Run pipelines from Databricks UI
4. Query Gold tables or materialized views
5. Build dashboards in Databricks SQL or Power BI

---

## ğŸ‘¤ Author
**Prashant Garg**  
Data Engineering | Databricks | Azure
